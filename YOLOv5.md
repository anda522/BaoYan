# YOLOv5火焰检测

# 变化阶段

YOLO是基于深度学习和卷积神经网络的单阶段通用目标检测算法，把目标检测问题转化为回归问题，不需要经过提取候选框的冗余问题。

## v1

- 算法思想

模型如何训练：在训练集上，我们首先要对图像进行标注，画出检测目标的ground truth，模型就是要将预测结果尽量拟合这个ground truth，使得损失函数最小化。之前将标注好了的图像分成S×S个（v1是7×7）网格（grid cell），对于ground truth的中心点，其中心点落在哪个grid cell中，就由这个grid cell生成的bounding box去负责拟合这个ground truth，因为每一个grid cell都会生成2个bounding box，那么就由这2个中的其中一个去负责拟合ground truth，且每一个grid cell去预测一个物体，则最多预测7×7个物体，**这也是YOLOv1预测小目标和密集目标性能较差的原因。**

每一个grid cell都会生成2个bounding box，那究竟由哪个bounding box去拟合ground gruth呢？就是由bounding box和ground truth交并比IOU较大的那个bounding box决定，较小的bounding box就直接舍弃，较大的留下来通过损失函数来不断进行调整以满足拟合ground truth。

- Yolov1的预测阶段、后处理阶段

`预测阶段`：输入一张图像，先把图像分成S×S（7×7）的网格（grid cell），每个grid cell都生成2个预测框（bounding box），每个grid cell都包括2个bounding box和20个类别，每个bounding box又包含4个位置参数（x、y、w、h）和1个置信度参数c，在原论文中bounding box的粗细就表示了置信度c的大小，粗的表示置信度较高，细的表示置信度较低。同时，每个grid cell预测一组条件类别的概率，就是原论文中每个彩色的网格就是代表预测了哪个类别（用颜色代表不同的类别，用粗细来表示每个bounding box的置信度）。

`后处理阶段NMS`：对所有检测框（bounding box）按置信度分数从大到小排序，第一个框作为起始框，剩下的框判断与第一个框是否相交，计算交并比（IoU），大于0.5（设置的阈值）删去，小于的不删

`NMS只存在于预测阶段，训练阶段是不需要NMS的`，因为在训练阶段每个bounding box都很重要。

- 网络结构

YOLOv1网络结构包括24层卷积层用来提取图像的特征，2层全连接层回归得到7×7×30（1+4+1+4+20）的张量。

网络结构大概如下：输入的是448×448×3通道的图像，就是RGB图像，然后用64个卷积核大小是7×7以步长为2进行卷积，然后是2×2最大池化，步长为2，然后是192个3×3卷积核进行卷积，然后再2×2最大池化，步长为2，然后后面就这样以一种级联的方式下去，最后获得一个7×7×1024维的张量，把它拉平喂到一个4096维的全连接层，输出一个4096维的向量，再把这个向量喂到一个1470维度的全连接层，输出一个1470维的向量，再把这个1470维度的向量reshape一下变成7×7×30的张量，所有预测框的坐标和类别都在这个7×7×30的张量里面。 

- 激活函数

YOLOv1中其最后一层使用的是线性激活函数，其他层使用的是LeakyReLU激活函数（$f(x)=x & x > 0 \\ f(x)=0.1x & x<0$），传统的ReLU函数是 $x<0$ 时 $f(x)=0$ 。

- 损失函数

使用的损失函数为**平方和误差**，是回归问题的损失函数，YOLO问题是把目标检测问题看做回归问题进行解决的，回归问题需要预测连续的值，所以把预测的值和标签的值的差作为损失函数。

但是还要加强定位误差损失，削弱不包含ground truth的预测框的confidence损失，在YOLOv1原论文中，对于负责检测物体的bounding box权重是5，不负责的权重是0.5。

优点：单阶段，速度非常快

缺点：

- 没有Batch Normalize
- 对小目标检测效果不好
- 对拥挤物体检测效果不好

## v2

在每一个卷积之后增加了BN层，提高网络训练速度，加快了收敛。

批规范化通过对每一层的输入进行归一化，使得每一层的输入分布更加稳定，从而缓解了梯度消失和梯度爆炸的问题，使得网络更容易训练。

具体来说，批规范化会对每个 mini-batch 的数据进行归一化操作，即将每个特征的数据减去均值，再除以标准差，从而使得数据的分布接近标准正态分布。同时，为了保证网络的表达能力，批规范化会引入两个可学习的参数：缩放因子（scale）和偏移量（shift），它们将对归一化后的数据进行缩放和平移操作，从而使得网络可以学习到更加丰富的特征表示。批规范化的具体计算公式为：
$$
\mathrm{BN}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
其中，$\gamma$ 和 $\beta$ 是可学习的参数，$\mu$ 和 $\sigma$ 分别是 mini-batch 中每个特征的均值和标准差，$\epsilon$ 是一个小的常数，用于避免除零错误。

---

Yolov1是直接预测目标框，Yolov2是基于anchor预测目标框

---

Backbone：主要使用了`Darknet-19`这个结构

`Backbone主干网络`：大多时候指的是提取特征的网络；

`Neck`：放在Backbone和Head之间是为了更好的利用Backbone提取的特征；

`Head`：获取网络输出内容的网络，利用之前提取的特征做出预测

## v3

YOLOv3主干网络BackBone：`Darknet-53`网络结构（因为有53个卷积层）

> 关于残差网络：
>
> 残差网络的提出是为了解决深层网络出现的退化问题，我们知道神经网络的每一层对应提取到不同层次的特征信息，网络越深的时候，提取到的信息也越丰富。而深度增加之后，会产生梯度消失或者梯度爆炸的问题，传统的解决办法是**数据初始化和BN层**，虽然解决了梯度消失和爆炸的问题，但也产生了别的问题，即网络的性能退化了，虽然深度增加但错误率却上升了。而残差网络，可有效解决此问题，同时也解决了梯度的问题。

**YOLOv3中正负样本的定义**：YOLOv3中就不再看中心点落在哪个grid cell（网格）里面就由哪个grid cell预测了，而是看谁的anchor与待检测目标的grouth truth的iou值最大，才由那个anchor去预测，也就是在YOLOv3里面正样本是指与grouth truth的iou值最大的预测框或anchor，对于和grouth truth有iou值且大于设定iou阈值（原论文中设为0.5）但不是最大值的预测框或anchor，就忽略它们；对于小于设定iou阈值的预测框或anchor而言就是负样本。

此外，在原论文中也提到，对于当前的bounding box，若其不是正样本的话，则它既没有定位损失，也没有类别损失，仅仅只有置信度损失（confidence loss）。

![](https://img-blog.csdnimg.cn/1441cfd76ff04278a72c8e70d080606e.png)



![](https://img-blog.csdnimg.cn/fee7ae76c7b340d0913ef7f382ed1e1f.png)

---

YOLOv3的损失函数主要分为三个部分：`目标定位偏移量损失`，`目标置信度损失`以及`目标分类损失`
$$
Loss=a \times Loss_{obj} + b \times Loss_{rect} + c \times Loss_{clc}
$$
坐标损失采用的是误差的平方和，类别损失采用的是二值交叉熵

---

Backbone（骨干网络）：用于提取特征

Neck（颈部网络）：用于汇总、融合不同尺度的特征，类似于FPN（特征金字塔）

Head（输出头）：获得各个尺度目标检测的预测结果（因为Backbone骨干网络是全卷积网络，可以兼顾任意尺度的输入，则可以输入32倍的任意尺度图像。所以对于YOLOv3来说，输入的图像越大，则单张输出的特征数也越多。）

## v4

Yolov4损失函数采用了`CIoU Loss`，此时可以说一下CIoU Loss的发展历程。$IoU - GIoU - DIoU - CIoU$

> **为什么CIOU这么优秀，却只在损失函数中使用CIoU-loss，而在NMS中使用的却是DIoU-NMS？**
>
> 答：因为CIOU需要真实框和预测框，在损失函数即训练阶段，是有标签的真实框的，而在NMS即测试推理阶段，是没有标签真实框的。

YOLOv4中采用SPP（结构见下图，就是一堆池化的组合）**作用：加速模型对核心特征的提取，提升模型的感受野。**

![](https://www.itbaizhan.com/wiki/imgs/image-20220517133905120.png)

**感受野（Receptive Field）**，指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。

Neck颈部网络中：使用了`FPN`和`PAN`，它把不同尺度的特征进行汇总整合，增强了模型对不同大小、不同尺度目标的识别能力。

FPN特征金字塔通过融合高底层特征，提升了不同尺度特别是小尺度的目标检测效果，而PAN在FPN特征金字塔的基础上，进一步增强了自底向上的特征增强。（原因：因为底层神经网络主要提取边缘、轮廓、颜色等底层视觉特征，而这些特征往往与目标的边缘和轮廓等密切相关，因此PAN-Net自底向上的特征增强，可以让顶层特征图充分共享到网络底层提取到的底层视觉特征，提升大目标的检测效果。）

## v5

Yolov5和v4的网络结构相似，较新的是提出了Focus模块

Focus结构：把长度和宽度方向的空间维度信息切片并堆叠至通道维度，长和宽缩小为原来的一半，通道数变成原来的4倍，节省计算量。

Focus和SPP都是为了增大感受野的操作，不过Focus是通过切片来做完整的特征提取，而SPP是通过最大池化来提取核心特征。

---

**自适应锚框计算：**

在YOLO算法中，针对不同的数据集，都会有初始设定长宽的锚框，且不会改变了，YOLOv2有5个，YOLOv3和YOLOv4是9个。

但在YOLOv5中网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框`ground truth`进行比对，计算两者差距，再反向更新，迭代网络参数。

所以YOLOv5在训练的过程中，每次训练时都会自适应的计算不同训练集的最佳锚框值，当然，这个功能也是可以在代码中将自动计算锚框功能关闭。

---

`输入端：Mosaic数据增强、自适应锚框计算、自适应图片缩放`

`Backbone：Focus结构，CSP结构`

`Neck：FPN+PAN结构`

`Prediction：GIOU_Loss`

## 参考

[1] yolo相关面试回答：https://blog.csdn.net/sazass/article/details/126990964

[2] 面试系列总结：https://blog.csdn.net/qq_45445740/article/details/120265713

[3] 目标检测总结：https://zhuanlan.zhihu.com/p/619025023

# 数据集相关

## 1 数据集收集

- 各个大学计算机视觉实验室的官方网站
- Github
- 网络爬虫

https://sjcj.nuaa.edu.cn/sjcjycl/article/html/202001003

## 2 数据集标准

- 每个类的图像。>= 1500 张图片。
- 每个类的实例。≥ 建议每个类10000个实例（标记对象）
- 图片形象多样。必须代表已部署的环境。对于现实世界的使用案例，我们推荐来自一天中不同时间、不同季节、不同天气、不同照明、不同角度、不同来源（在线采集、本地采集、不同摄像机）等的图像。
- 标签一致性。必须标记所有图像中所有类的所有实例。部分标记将不起作用。
- 标签准确性。
- 标签必须紧密地包围每个对象。对象与其边界框之间不应存在任何空间。任何对象都不应缺少标签。
- 标签验证。查看train_batch*.jpg 在 训练开始验证标签是否正确
- 背景图像。背景图像是没有添加到数据集以减少 False Positives（FP）的对象的图像。我们建议使用大约0-10%的背景图像来帮助减少FPs（COCO有1000个背景图像供参考，占总数的1%）。背景图像不需要标签。

300轮，22张验证图片，有多张图片重复，效果较好。

# 网络结构

![](https://img-blog.csdnimg.cn/4e7a390fcf364e2d84346f1689e12aa6.png)

![image-20230331212036242](YOLOv5/image-20230331212036242.png)

> https://blog.csdn.net/weixin_43427721/article/details/123613944

yaml文件中的网络组件不需要进行叠加，只需要在配置文件中设置number即可。

CBS模块其实没什么好稀奇的，就是Conv+BatchNorm（归一化）+SiLU（用sigmoid激活）

## CSP

**CSP即backbone中的C3，因为在backbone中C3存在shortcut，而在neck中C3不使用shortcut，所以backbone中的C3层使用CSP1_x表示，neck中的C3使用CSP2_x表示。**

![](https://img-blog.csdnimg.cn/cbfea02735b2408193de3eb3be2ad396.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5b-X5oS_5peg5YCm,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

为了能让深层次的网络模型达到更好的训练效果，残差网络中提出的残差映射替换了以往的基础映射。对于输入x，期望输出H(x)，网络利用恒等映射将x作为初始结果，将原来的映射关系变成F(x)+x。与其让多层卷积去近似估计H(x) ，不如近似估计H(x)-x，即近似估计残差F(x)。因此，ResNet相当于将学习目标改变为目标值H(x)和x的差值，后面的训练目标就是要将残差结果逼近于0。
残差模块有什么好处呢？

> 1.梯度弥散方面。加入ResNet中的shortcut结构之后，在反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度，这相当于把每一个block中向前传递的梯度人为加大了，也就会减小梯度弥散的可能性。
> 2.特征冗余方面。正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。这显然会发生类似欠拟合的现象。加入shortcut结构，相当于在每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。

**在resnet中，人们可以使用带有shortcut的残差模块搭建几百层甚至上千层的网络，而浅层的残差模块被命名为Basicblock（18、34），深层网络所使用的的残差模块，就被命名为了Bottleneck（50+）。**

## SPPF

![](https://img-blog.csdnimg.cn/7136b20c799342f6a458d42939470e64.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5b-X5oS_5peg5YCm,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

这一模块的主要作用是对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。

## Neck

https://blog.csdn.net/weixin_43427721/article/details/123653669

![](https://img-blog.csdnimg.cn/bfc40cf6ffb44925b7bc0ade1afe913b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5b-X5oS_5peg5YCm,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

Neck的网络结构设计也是沿用了**FPN+PAN的结构**

FPN就是使用一种 自顶向下的侧边连接在所有尺度上构建出高级语义特征图，构造了特征金字塔的经典结构。

PAN的结构也不稀奇，FPN中间经过多层的网络后，底层的目标信息已经非常模糊了，因此PAN又加入了自底向上的路线，弥补并加强了定位信息，也就是上图中的b。

高层用来预测大目标，底层用来预测小目标，小目标对应小的anchor

# 相关技术

## 1 Mosaic数据增强

Mosaic数据增强技术采用了四张图片的随机缩放、随机剪裁、随机排布的方式对数据进行拼接，相比CutMix数据增强多用了两张图片。在目标识别过程中，要识别的目标有大目标、中等目标、小目标，并且三种目标的占比例不均衡，其中，小目标的数量是最多的，但是出现的频率很低，这种情况就会导致在bp时对小目标的优化不足，模型正确识别小目标的难度比识别中、大目标的难度要大很多，于是对于小目标来说很容易出现误检和漏检的情况。Mosaic数据增强技术做出改进后，上述的问题得到有效的解决。

- 丰富了数据集，采用“三个随机”的方式对数据进行拼接丰富了检测的数据集，尤其是随机缩放增加了很多小目标，克服了小目标的不足，让网络的鲁棒性得到提高；
- 减少GPU的使用，在Mosaic增强训练时，四张图片拼接在一起，GPU可以直接计算四张图片的数据，让Mini-batch的大小减少了很多，这使得一个GPU就可以达到比较可观的效果。

## 2 自适应anchor

一开始会先计算Best Possible Recall (BPR)

再在kmean_anchors函数中进行k 均值和遗传学习算法更新anchors。

自适应anchor是check＿anchors函数通过遗传算法与Kmeans迭代算出的最大可能召回率的anchor组合。在网络模型的训练过程中，网络在初始化的锚框的基础上输出预测框，然后与真实框groundtruth进行对比，计算两个框之间的差值，再根据差值进行反向更新，迭代网络参数，最后求出最佳的锚框值。自适应的anchor能够更好地配合网络训练，提高模型的精度，减少对anchor的设计难度，具有很好的实用性。

## 3 自适应图片缩放

- 图片预处理

- 如果是简单的使用resize，很有可能就造成了图片信息的丢失
- 在图片比例一致的情况下（长宽的收缩比例应该采用相同的比例），像素填充，保证长宽均被32整除

为了提高模型的推理速度，YOLOv5提出自适应图片缩放，根据长宽比对图像进行缩放，并添加最少的黑边，减少计算量。

该方法是用缩放后的长边减去短边再对32进行取余运算，求出padding。

在训练时并没有采用缩减黑边的方法，该方法只是在测试模型推理的时候才使用，这样提高了目标检测的准确率和速度。

## 4 Focus结构

该结构采用切片操作，将特征切成四份，每一份将当成下采样的特征，然后在channel维度进行concat。例如：原始608*608*3的数据图片，经过切片操作先变成304*304*12的特征图，再经过一次32个卷积核的卷积操作，变成304*304*32的特征图。

## 5 CSP结构

YOLOv5中的CSP[5]结构应用于两处，一处是CSP1＿X结构应用于Backbone的主干网络中，另一处的CSP2＿X结构应用于Neck中，用于加强网络的特征融合的能力。CSPNet主要从网络结构设计的角度解决推理中从计算量很大的问题。该结构的优点有：1)增强CNN的学习能力，使得模型在轻量化的同时保持较高的准确性；2)减低计算的瓶颈问题；3)减低内存的分险。

## 6 FPN+PAN结构

FPN：

- 顶层特征上采样后和底层特征融合，每层独立预测。

这个结构是FPN和PAN的联合。FPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图，而PAN正好与FPN的方向是相反的方向，它是自底向上地采取特征信息。两个结构各自从不同的主干层对不同的检测层进行参数聚合。两个结构的强强联合让得到的特征图的特征更加明显和清楚。

## 7 Bounding box的损失函数

默认使用的是CIoU，不是GIoU，不是DIoU，是CIoU

Bounding box损失函数增加了相交尺度的衡量方式，有效缓解了当两个框不相交和两个框大小完全相同的两种特殊情况。因为当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近的时候，此时的损失函数不可导；两个框大小完全相同，两个IOU也相同，IOU＿LOSS无法区分以上两种特殊情况。

## 8 nms非极大值抑制

>  后处理技术

Non-Maximum Suppression

在目标检测过程的后续处理中，对于大量的目标框的筛选问题，通常会进行nms操作，以此来达到一个不错的效果。YOLOv5算法同样采用了加权的nms操作。

步骤：对所有检测框（bounding box）按置信度分数从大到小排序，第一个框作为起始框，剩下的框判断与第一个框是否相交，计算交并比（IoU），大于0.5删去，小于的不删

# 损失函数

## 1 CIOU LOSS

损失函数计算原理：https://zhuanlan.zhihu.com/p/458597638

分为矩形框损失(lossrect)、置信度损失（lossobj）、分类损失(lossclc)

因此yolov5网络的损失函数定义为：

Loss=a\*lossobj+ b\*lossrect+ c\*lossclc

> 也即总体损失为三个损失的加权和，通常置信度损失取最大权重，矩形框损失和分类损失的权重次之
>
> yolov5使用**CIOU loss**计算矩形框损失，置信度损失与分类损失都用**BCE loss**计算

$$
IoU = \frac{A \cap B}{A \cup B} = \frac{S_1}{S_2} \\
IoU \ Loss = 1 - IoU
$$


$$
GIoU = IoU - \frac{S_3 - S_2}{S_3} \\
GIoU \ Loss = 1 - GIoU
$$




> $S_3$ ：包围两个矩形框的最小矩形的面积， $S_2$ ：矩形相并的面积，$S_1$：面积交 
>
> 当两个矩形框完全没有重叠区域时，无论它们距离多远，它们的IOU都为0。这种情况下梯度也为0，导致无法优化。为了解决这个问题，GIOU又被提了出来。

$$
DIoU = IoU - \frac{\rho ^ 2}{c ^ 2} \\
\rho:矩形框A,B中心点的距离;c:最小外接矩形对角线的长度 \\
DIoU Loss = 1 - DIoU
$$



> GIOU虽然把IOU的问题解决了，但它还是基于面积的度量，并没有把两个矩形框A、B的距离考虑进去。
>
> DIOU把矩形框A、B的中心点距离ρ、外接矩形框（包围两个矩形框的最小矩形）的对角线长度c都直接考虑进去

$$
CIoU = IoU - \frac{\rho ^ 2}{c ^ 2} - \alpha v = DIoU - \alpha v \\
v = \frac{4}{\pi ^ 2} \Big( \mathop{arctan} \frac{w_{gt}}{h_{gt}} - \mathop{arctan} \frac{w_p}{h_p} \Big) \\
CIoU Loss = 1 - CIoU
$$



> DIOU把两个矩形框A、B的重叠面积、中心点距离都考虑了进去，但并未考虑A、B的宽高比。
>
> ρ为框A和框B的中心点距离，c为框A和框B的最小包围矩形的对角线长度，v为框A、框B的宽高比相似度，α为v的影响因子
>
> $w_l, h_l$ 标注框的宽和高，$w_p, h_p$预测框的宽和高

## 2 BCE LOSS

`BCE LOSS` 也就是交叉熵损失函数。
$$
Loss_{BCE}(z, x, y) = - L(z, x, y) \times log_2 P(z, x, y) - (1 - L(z, x, y)) \times log_2 (1 - P(z, x, y)) \\
0 \leq z \lt 3, 0 \leq x \lt 80, 0 \leq y \lt 80
$$



置信度标签为矩阵L（采用CIoU），预测置信度为矩阵P

**对mask为true的位置不直接赋1，而是计算对应预测框与目标框的CIOU，使用CIOU作为该预测框的置信度标签，当然对mask为false的位置还是直接赋0**。
$$
L_{obj} = \frac{1}{num[mask=true]} \sum _{\substack{mask=True\\0 \leq z \lt 3 \\ 0 \leq x \lt 80 \\ 0 \leq y \lt 80}} Loss_{BCE}(z, x, y) \\
L_{noobj} = \frac{1}{num[mask=False]} \sum _{\substack{mask=False\\0 \leq z \lt 3 \\ 0 \leq x \lt 80 \\ 0 \leq y \lt 80}} Loss_{BCE}(z, x, y) \\
L_{obj80} = \alpha \times L_{obj} + (1 - \alpha) \times L_{noobj}
$$
分类概率损失：

分类类别 $c$
$$
Loss_{BCE}(z, x, y, c) = - L(z, x, y, c) \times log_2 P(z, x, y, c) - (1 - L(z, x, y, c)) \times log_2 (1 - P(z, x, y, c)) \\
0 \leq z \lt 3, 0 \leq x \lt 80, 0 \leq y \lt 80, 0 \leq c \lt 3
$$
分类损失：
$$
L_{cls80} = \frac{1}{3 \times num[mask=true]} \sum \limits_{\substack{mask=True\\0 \leq z \lt 3 \\ 0 \leq x \lt 80 \\ 0 \leq y \lt 80 \\ 0 \leq c \lt 3}} Loss_{BCE}(z, x, y, c)
$$

---

在 `YOLOV5` 代码中，并没有采用传统的交叉熵损失函数，而是进行了相关改造。代码如下：

```python
def forward(self, pred, true):
    loss = self.loss_fcn(pred, true)
    pred = torch.sigmoid(pred)  # 需要先进行sigmoid处理
    # 仅减少缺失标签（missing label）对损失的影响，即模型错误地将某个标签预测为不存在。
    # pred = 1, true = 0, dx = 1, 不应该有过多的惩罚，权重因子为0，没有取绝对值
    dx = pred - true
    # dx = (pred - true).abs()  # 会减轻pred和gt差异过大而造成的影响
    alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))
    loss *= alpha_factor
    return loss.mean()
```

---

使用交叉熵损失函数，这会导致模型对正确分类的情况奖励最大，错误分类惩罚最大。如果训练数据能覆盖所有情况，或者是完全正确，那么这种方式没有问题。但事实上，这不可能。所以这种方式可能会带来泛化能力差的问题，即过拟合。

可以采用 `标签平滑` 策略进行缓解，即对原来的标签进行改动，添加一个平滑参数 $\alpha$ ，分类正确时的概率为 $p = 1 - 0.5 \times \alpha$ ，分类不正确时的概率为 $p = 0.5 \times \alpha$ ，也就是对分类准确做了惩罚，抑制了过拟合的效果。

> 交叉熵损失函数介绍：https://blog.csdn.net/qq_38253797/article/details/116225218
>
> 损失函数 `loss.py` 源码文件解读：https://blog.csdn.net/qq_38253797/article/details/119444854
>
> 建议参考，里面有 `Focal Loss` 损失函数介绍。

# 评估指标

> 参考：https://zhuanlan.zhihu.com/p/619025023

1. 准确率（Accuracy）：指模型正确识别目标的比例。
2. 精度（Precision）：指模型预测为目标的样本中真正是目标的比例。
3. 召回率（Recall）：指模型检测出的目标的样本中真正是目标的比例。
4. F1值（F1-Score）：综合考虑精度和召回率的指标，用于从综合的角度评价模型的性能。
5. 平均精度均值（mAP）：是评估目标检测模型性能的一种重要指标，主要基于目标检测中每个类别的精度-召回曲线来计算。mAP越高，代表模型越准确。如COCO数据集中的mAP，PASCAL VOC中的mAP等。
6. IoU（Intersection over Union）：是指检测框与真实框之间的相交面积占两者并集面积的比例，通常用于度量目标检测算法中物体定位准确性。当IoU值越高时，检测结果越准确


$$
ACC = \frac{TP + TN}{N_P + N_N}\\
P = \frac{TP}{TP + FP} \\
R = \frac{TP}{TP + FN} \\
F1\_Score = 2 \times \frac{P \times R}{P + R} \\
AP = \int_0^1 P(R) dR \\
mAP = \frac{1}{n} \sum \limits_{i = 1}^{n} \int_0^1 P(R)dR
$$


# 缺陷

- 图片或视频像素太低导致识别误差，有些火焰无法识别出来。（数据基本上都是高像素的图片）
- 训练集较少，只有500张左右，验证集较少，只有35张
- 训练集重复数据，同类数据较多
- 大目标有时难以全部识别，可能会识别部分
- 小目标有时无法检测，可能目标非常小，难以检测到
- 正负样本数量不均衡，或者负样本数量较少
- 在自然场景下，天梯情况，光照强度，背景干扰等因素影响火焰识别的准确性

# 相关面试题

## 1 目标检测两阶段和一阶段的核心区别

目标检测算法从阶段上分为两种： **一阶段和二阶段**。

一阶段网络的核心是对于输入图像通过网络直接回归出目标大小、位置和类别。常见算法有YOLO，SSD

二阶段的核心是通过第一阶段的网络回归出目标框的大概位置、大小及是前景的概率，第二阶段是通过另一个网络回归出目标框的位置、大小及类别。常见算法有R-CNN（区域卷积神经网络）

## 2 YOLOv5检测过程

- **首先**判断每个预测框的预测置信度是否超过设定阈值，若超过则认为该预测框内存在目标，从而得到目标的大致位置。
- **接着**根据非极大值抑制算法对存在目标的预测框进行筛选，剔除对应同一目标的重复矩形框
- **最后**根据筛选后预测框的分类概率，取最大概率对应的索引，即为目标的分类索引号，从而得到目标的类别。

## 3 项目的创新点是什么

1. 数据集正负样本不均衡，负样本较少（同样也是项目难点）

> 正样本指的是火焰样本，负样本指的是非火焰样本。
>
> 正负样本不均衡会导致：
>
> - 过拟合问题：训练过度关注数量多的样本，在训练集中表现良好，数据泛化能力差
> - 学习偏差，模型精度变低：模型更倾向于预测数量更多的类别，无法充分学习数量较小的类别，导致性能偏差
> - 训练效率低下：过多的负样例提供了无用的训练信号

解决：

- 数据集增加大量不存在火焰目标的背景图，同时增加大量易混疑似火焰数据集，如朝霞，夕阳，室内灯光，车灯等，提高模型的泛化性能
- 过采样（对少数类别复制或合成新的样本）和欠采样（对于多数类别的样本进行随机删除）

2. 数据集场景涵盖丰富，质量较高

数据集涵盖室内室外，白天黑夜，森林，工厂，房屋，公路，山地等不同场景

3. 数据集数量太少（使用OpenCV等库进行实现）

> 随机缩放、随机裁剪、随机擦除、随机翻转、随机旋转

- 采用数据增强技术：将4张图片或9张图片拼接起来，相当于训练了4张或9张小图，克服了小目标识别较难的问题。

- 对图片进行仿射变换：随机旋转，平移，缩放，错切

随机水平翻转

## 4 你采用哪些策略来收集标注数据集，数据质量数量对模型影响

- 在网上搜集公开数据集，如GitHub，大学实验室官网
- 爬虫程序爬取含有火焰场景的视频帧，爬虫图片
- 实际制作数据集，设置不同的场景和拍摄角度，光照等条件

数据质量评估：我通过统计不同场景下**火焰的大小以及在图像中的分布**，确保每种情况都有足够的数据来支撑。

## 5 通过数据预处理实现了哪些效果

- 修正部分数据的标注错误，使模型收敛更快
- 数据增强（在前向传播过程前进行多尺度和翻转操作，并将处理后的多个输出结果拼接返回）策略，扩充数据集规模
- 图像裁剪，保留目标部分，降低无关干扰
- 调整目标色彩、大小等
- 清理重复或低质量数据，提升数据集整体质量

## 6 项目的难点，如何解决的

难点主要在于数据分布不平衡，正负样本不均衡：

- 非火焰样本较少，火焰样本较多

> 解决：
>
> - 数据集增加大量不存在火焰目标的背景图，同时增加大量易混疑似火焰数据集，如朝霞，夕阳，室内灯光，车灯等，提高模型的泛化性能

- 火焰样本小目标较少，大目标较多

> 解决：
>
> - 计算样本权重，加大模型对小样本的关注问题，缓解数据不均衡问题
> - 增大小样本数据量，采用数据增强方法（随机缩放，增加了小目标），使用缩放旋转等方式增强数据集

- 简单样本小，困难样本多（对于预测的难易度）

> 解决：
>
> - 采用 `Focal Loss` 损失函数，使模型更加专注复杂样本的学习。通过降低简单样本（数量多）的损失权重，使损失函数更加专注于困难样本（数量），防止简单样本主导整个损失函数。
>
> 参考： `Focal Loss` 总结 https://blog.csdn.net/qq_38253797/article/details/116292496

# 相关参考

> [3] 和 [4] 我认为是非常详细的讲解YOLOv5的文章，值得一看！

[1] 基于YOLOv5的行人车辆检测论文解读： https://qianxu.run/2021/07/02/YOLO-paper/

[2] YOLOv5上融合多特征的实时火焰检测方法： http://manu46.magtech.com.cn/Jweb_prai/CN/abstract/abstract12396.shtml

[3] YOLOv5代码教程（以OneFlow为后端的YOLOv5目标检测项目）：https://start.oneflow.org/oneflow-yolo-doc/index.html

[4] YOLOV5-5.x源码讲解：https://blog.csdn.net/qq_38253797/article/details/119043919
